# Adapting Interpretability Techniques to Generative Diffusion Models
Interpretable models play a pivotal role in understanding the inner workings of complex black box models. Among the established techniques, LIME and SHAPley Values have gained prominence for their effectiveness. However, their application to generative diffusion models remains underexplored with regard to relating specific text in the prompt to generated imagery. Further, knowing what specifically in the image is contributing to which tokens would be useful in contexts of identifying misrepresentation of semantic meanings. In this project, we bridge this gap by adapting SHAP and LIME interpretability techniques to generative diffusion models and then working backwards from the generated models to highlight key imagery associated with each term. Specifically, we aim to elucidate the correspondence between words and objects within images generated by diffusion models. To achieve this, we analyze the intermediate steps of the diffusion process, which can be achieved with open source models. Alternatively, we use SOTA image segmenting to understand objects in the scene that could have direct correlations to specific phrases within the input prompt. This enables us to pinpoint the significance of words in relation to specific concepts depicted in the image. Our findings not only highlight the importance of individual words but also provide insights into the underlying concepts represented by these words. By offering a deeper understanding of the importance of tokens in a prompt and the interpretability of shapes within images, our approach contributes to advancing the state-of-the-art in explainable generative models.
## Background
Generative text-to-image models such as Stable Diffusion have become highly popular in recent times due to their ability to generate high quality images from natural language prompts. These models are trained on large natural language and image datasets, and are largely black box in nature which therefore motivates an approach to explainability that is accessible using black-box only techniques. The images produced by the model are a result of the prompt inputted by the user, and how the model perceives the prompt and the composition of images. This paper explores how the diffusion models detects the importance of different parts of a prompt, and also which parts of an image are relevant to each token in the prompt.

The importance of difference parts of a prompt and different parts of the image as it realtes to the prompt can be seen as a feature attribution problem. The feature attribution problem refers to the setup: given a model $f$ and an input $x$ of finding the subset of features of a model that contribute the most to prediction f(x).

We adapt three explainability methods, Shapley values, LIME, and Saliency maps, all of which are popularly applied to classical machine learning models to interpret the outputs of the stable diffusion model, which is a text-to-image generative model. We then use the images that are generated to reanalyze this problem on a per-image-idea basis.
### LIME ALgorithm
The LIME algorithm is a black-box way of understanding which part of the input of a model are able to relate to specific aspects of the output. LIME itself stands for Local interpretable model-agnostic explanations. This is a linear approximation that strives to subtly manipulate the input in a computationally acceptable level, so that it is understandable which movements cause which outputs. This is especially important when attempting to understand why it is that a model is acting a specific way and can be helpful in debugging a model or getting better guarantees on thought process when decisions are critical and not merely accuracy-based. This means that LIME does not need to have access to the training data and essentially creates a mini-data-set of its own with slight perturbations and observation of the output set. \parencite{InterpretableML} In order to use LIME in the first problem of the paper, we are interested in natural language input. We can treat each of the tokens as a perturbable feature that can be used in order to move in a region space around the original natural language input. LIME can also be used for images which is important in understanding which parts of the image are contributing to an understanding of a specific phrase. This would be far too computationally expensive to alter random pixels and therefore 'superpixels' are used to perturb the input. This is essentially just groups of similar pixels within an image that can be turned to a neutral color that would theoretically and ideally not have a new large meaning on the predicted output. Because linear models of random perturbations can result in overfitting that is not replicated in images that are epsilon away, it is important to understand the imperfection of this technique and compare results with those generated by SHAP or Saliency maps. 
### SHAP Algorithm
Shapley values are an explanation method adapted from cooperative game theory to provide a solution to the feature attribution problem. The cooperative game shap consists of:
* a set of players $D = \{ 1, ... d \}$
* a coalition $S \subseteq D$ that represents a game by specifying the set of players
* a characteristic function $v$, that represents the payoff of the given set. 
The Shapley value is a technique for allocating credit to players in the cooperative game. For $G$ denoting the set of games on $d$ players, the shapley values are each:

$\phi: G \mapsto R$ 

For a game $v$, Shapley values are $\phi_{1}(v), ... \phi_{d}(v)$

The following equation is used to calculate $\phi_{i}(v):$

$\phi_{i}(v) = \sum_{S \subseteq D \symbol{92} i}{\frac{|S|!(d-1-|S|)!}{d!} [v(S \cup \{ i \}) - v(S)]}$

This calculates the average contribution of each player across all player orderings. $[v(S \cup \{ i \}) - v(S)]$ is the marginal contribution of a player to subset S, and $\frac{|S|!(d-1-|S|)!}{d!}$ is the probability of that coalition. 

The Shapley values are applied to machine learning by considering model features as players and some quantification of model behavior as the payoff e.g. the prediction or the loss huggingface. The Shapley values are used to quantify each feature's impact on a given prediction.

The adaptation of Shapley values to generative models such as Stable Diffusion is explained later.

## Evaluation Methods
We evaluate the results of the Shapley and LIME values using two methods that we have developed:
### Percentage of Ranks Aligned
The dataset contains importance scores associated with each token of a prompt, as ground truth i.e. we rank the tokens in each prompt in order of importance. This evaluation metric is the simple percentage of ranks determined by the Shapley or LIME value aligning with the ground truth ranks. 

Percent of Ranks Aligned = $\frac{\text{No. of tokens with correctly assigned ranks}}{\text{No. of tokens in prompt}}$

Example: calculating the Percent of Ranks Aligned for Shapley values of \textit{ a cat's house}:

| Tokens in *a cat's house* | a    | cat's | house |
|---------------------------|------|-------|-------|
| Ground truth score        | 0.05 | 0.45  | 0.5   |
| Ground truth rank         | 3    | 2     | 1     |
| Shapley score             | 0.13 | 0.59  | 0.28  |
| Shapley rank              | 3    | 1     | 2     |
| Ranks are aligned         | T    | F     | F     |

### Root Mean Squared Error
The Root Mean Squared Error (RMSE) calculates the root mean squared difference between ground truth importance scores and importance scores outputted by the explainability method of each token in the prompt

Root Mean Squared Error = $\sqrt{\frac{\sum_{i=1}^N{(x_i - \hat{x_i})^2}}{N}}$

where $x_i$ is the ground truth importance score, $\hat{x_i}$ is the importance score of the explainability method (normalized to add up to one), and N is the number of tokens.

\end{center}

An example calculating the Root Mean Squared Error for Shapley values of \textit{ a cat's house} is shown below:


| Tokens in *a cat's house* | a     | cat's | house |
|---------------------------|-------|-------|-------|
| Ground truth score        | 0.05  | 0.45  | 0.5   |
| SHAP score                | 0.13  | 0.59  | 0.28  |
| Squared Error             | 0.0064| 0.02  | 0.05  |

Root Mean Squared Error = $\sqrt{\frac{0.0064 + 0.02 + 0.05 }{3}} 
= 0.2728$

## Methods
### Token to Image Interpretability
The first question that the interpretability algorithms are able to answer is whether or not the given token in the input is going to have a drastic change on the image as a whole. This 'change' for images is a difficult thing to measure in a context in which one must compare two elements of an image to determine which plays a larger role in the general meaning of the input. There are several ways in which these results can be measured and each of these comes with contextual upsides and downsides
* $\textbf{CLIP similarity}$ is a way to move both of the images into a smaller latent space that can measure similarity by comparing the embeddings directly. It captures semantic similarities between images, even if visually dissimilar, by leveraging its pretrained knowledge. This has the benefit of being able to generally understand what semantic features of images are important, but it is not related to this task specifically. Still, this remains the best metric.
* $\textbf{VAE similarity}$ encodes images into a latent space, where similar images have nearby representations. Similarity is determined by measuring distances between these latent vectors similar to in CLIP, but this task is specifically for prompt to image similarities. While there are several datasets of this, we decided cultivating more specific similarities and the compute required to train the VAE would not have had significant enough improvement over CLIP, so we decided not to implement this technique.
* $\textbf{Latent Diffusion Embedding similarity}$ similar to the previous two methods, comparing the latent space. Notably this is within the specific model that is being used and requires open sourcing the model so that the intermediate modes can be seen. Further, this has the major disadvantage of not having similar spaces when similar objects are in different parts of the image (it does not extrapolate or embed to a sufficient degree).

#### LIME Approach
The LIME approach differs from classical methods in that the output of the model is an image rather than a classification with simple similarity. Further, implementing LIME to be able to use generative models required rewriting much of the algorithm to allow for the needed setup that this problem entailed. The figure below represents some of the differently generated models that can exist from the LIME generation. The main image is generated from the prompt "A Lamp with flowers", while the three images below are subsets consisisting from (in order from left to right) "Lamp with flowers", "A with flowers", "A lamp with". In practice, all of the subsets are chosen and weighted based off of how many of the other tokens are included in order to measure the difference between the original picture and the generated pictures. Due to compilation issues, this code must be manually completed in notebooks and cannot be run from the terminal the way other LIME libraries are able to function. This is big on the todo list as we continue this project into the summer!

#### Shapely Values Approach
The Shapley Values approach, similar to LIME, is adapted to generative models by adapting the nature of inputs and outputs. While in classical machine learning models, the model is trained on features and outputs a prediction, stable diffusion models take prompts as inputs and produce images as outputs. We adapt Shapley values to image-to-text generative models by assigning the following as inputs and outputs:


* Each token in a prompt is a player e.g. when the prompt is $\textit{a cat's house}$, the players are $\textit{a}$, $\textit{cat's}$, and $\textit{house}$.
* The complete coalition $D$ is the original prompt e.g. {a cat's house}, and the coalitions or subsets $S \subseteq D$ of the game  are all possible combinations of tokens in the same order e.g. a, cat's, house, a cat's, a cat's house, cat's house, a house. There are $2^d - 1$ subsets of a prompt (since we exclude the empty set).
* The image similarity of a subset of tokens $S$ to the original prompt $D$ is its payoff, or value $v(S)$. 

While there are methods to estimate Shapley values for games with larger values of d, this paper employs the original formula to calculate Shapley values:

$\phi_{i}(v) = \sum_{S \subseteq D \symbol{92} i}{\frac{|S|!(d-1-|S|)!}{d!} [v(S \cup \{ i \}) - v(S)]}$

The permutation-based algorithm is applied, that considers all possible orderings of tokens, and average marginal contributions across them for each token. The marginal contributions are normalized by dividing by the total of marginal contributions of each token to a prompt to get the Shapley value. 

### Image to Generator
Once we have an image generated and a description of the importance of tokens that went into the model as an input, the question still remains of what within the image relates to what tokens within the prompt. A motivating case for this could include wanting to make sure that when the model generates images that generated sections relate to the correct descriptors. In the case of radiology, one would want to know what in the image not only relates to the general prompt, but relates to the specific elements of the prompt. The same would go for many industries where precision about how to model views parts of the image could have large consequences. 

The first step in correlating parts of the image to the desired parts of the prompt is segmenting the input in meaningful ways. We analyze this in two different ways. The first is in terms of what the model is looking at which would make this technique useful in understanding the hierarchy within the models interpretation of the prompt. The second approach is in terms of object segmentation and allows for a more human interpretable approach where the onlooking can quickly recognize what 'things' in the image relate instead of general regions of the image. The two methods are further explained below.


