# Adapting Interpretability Techniques to Generative Diffusion Models
Interpretable models play a pivotal role in understanding the inner workings of complex black box models. Among the established techniques, LIME and SHAPley Values have gained prominence for their effectiveness. However, their application to generative diffusion models remains underexplored with regard to relating specific text in the prompt to generated imagery. Further, knowing what specifically in the image is contributing to which tokens would be useful in contexts of identifying misrepresentation of semantic meanings. In this project, we bridge this gap by adapting SHAP and LIME interpretability techniques to generative diffusion models and then working backwards from the generated models to highlight key imagery associated with each term. Specifically, we aim to elucidate the correspondence between words and objects within images generated by diffusion models. To achieve this, we analyze the intermediate steps of the diffusion process, which can be achieved with open source models. Alternatively, we use SOTA image segmenting to understand objects in the scene that could have direct correlations to specific phrases within the input prompt. This enables us to pinpoint the significance of words in relation to specific concepts depicted in the image. Our findings not only highlight the importance of individual words but also provide insights into the underlying concepts represented by these words. By offering a deeper understanding of the importance of tokens in a prompt and the interpretability of shapes within images, our approach contributes to advancing the state-of-the-art in explainable generative models.
## Background
Generative text-to-image models such as Stable Diffusion have become highly popular in recent times due to their ability to generate high quality images from natural language prompts. These models are trained on large natural language and image datasets, and are largely black box in nature which therefore motivates an approach to explainability that is accessible using black-box only techniques. The images produced by the model are a result of the prompt inputted by the user, and how the model perceives the prompt and the composition of images. This paper explores how the diffusion models detects the importance of different parts of a prompt, and also which parts of an image are relevant to each token in the prompt.

The importance of difference parts of a prompt and different parts of the image as it realtes to the prompt can be seen as a feature attribution problem. The feature attribution problem refers to the setup: given a model $f$ and an input $x$ of finding the subset of features of a model that contribute the most to prediction f(x).

We adapt three explainability methods, Shapley values, LIME, and Saliency maps, all of which are popularly applied to classical machine learning models to interpret the outputs of the stable diffusion model, which is a text-to-image generative model. We then use the images that are generated to reanalyze this problem on a per-image-idea basis.
### LIME ALgorithm
The LIME algorithm is a black-box way of understanding which part of the input of a model are able to relate to specific aspects of the output. LIME itself stands for Local interpretable model-agnostic explanations. This is a linear approximation that strives to subtly manipulate the input in a computationally acceptable level, so that it is understandable which movements cause which outputs. This is especially important when attempting to understand why it is that a model is acting a specific way and can be helpful in debugging a model or getting better guarantees on thought process when decisions are critical and not merely accuracy-based. This means that LIME does not need to have access to the training data and essentially creates a mini-data-set of its own with slight perturbations and observation of the output set. \parencite{InterpretableML} In order to use LIME in the first problem of the paper, we are interested in natural language input. We can treat each of the tokens as a perturbable feature that can be used in order to move in a region space around the original natural language input. LIME can also be used for images which is important in understanding which parts of the image are contributing to an understanding of a specific phrase. This would be far too computationally expensive to alter random pixels and therefore 'superpixels' are used to perturb the input. This is essentially just groups of similar pixels within an image that can be turned to a neutral color that would theoretically and ideally not have a new large meaning on the predicted output. Because linear models of random perturbations can result in overfitting that is not replicated in images that are epsilon away, it is important to understand the imperfection of this technique and compare results with those generated by SHAP or Saliency maps. 
### SHAP Algorithm
Shapley values are an explanation method adapted from cooperative game theory to provide a solution to the feature attribution problem. The cooperative game shap consists of:
* a set of players $D = \{ 1, ... d \}$
* a coalition $S \subseteq D$ that represents a game by specifying the set of players
* a characteristic function $v$, that represents the payoff of the given set. 
The Shapley value is a technique for allocating credit to players in the cooperative game. For $G$ denoting the set of games on $d$ players, the shapley values are each:

$\phi: G \mapsto R$ 

For a game $v$, Shapley values are $\phi_{1}(v), ... \phi_{d}(v)$

The following equation is used to calculate $\phi_{i}(v):$

$\phi_{i}(v) = \sum_{S \subseteq D \symbol{92} i}{\frac{|S|!(d-1-|S|)!}{d!} [v(S \cup \{ i \}) - v(S)]}$

This calculates the average contribution of each player across all player orderings. $[v(S \cup \{ i \}) - v(S)]$ is the marginal contribution of a player to subset S, and $\frac{|S|!(d-1-|S|)!}{d!}$ is the probability of that coalition. 

The Shapley values are applied to machine learning by considering model features as players and some quantification of model behavior as the payoff e.g. the prediction or the loss huggingface. The Shapley values are used to quantify each feature's impact on a given prediction.

The adaptation of Shapley values to generative models such as Stable Diffusion is explained later.

## Evaluation Methods
We evaluate the results of the Shapley and LIME values using two methods that we have developed:
### Percentage of Ranks Aligned
The dataset contains importance scores associated with each token of a prompt, as ground truth i.e. we rank the tokens in each prompt in order of importance. This evaluation metric is the simple percentage of ranks determined by the Shapley or LIME value aligning with the ground truth ranks. 

Percent of Ranks Aligned = $\frac{\text{No. of tokens with correctly assigned ranks}}{\text{No. of tokens in prompt}}$

Example: calculating the Percent of Ranks Aligned for Shapley values of \textit{ a cat's house}:

<div style="text-align: center;">
    <a href=img/RanksAlignedExample.png>
        <img src=img/RanksAlignedExample.png alt="IMAGE ALT TEXT HERE"/>
    </a>
</div>

\begin{tabular}{|l|l|l|l|}
\hline
Tokens in \textit{ a cat's house} & a    & cat's & house \\ \hline
ground truth score    & 0.05 & 0.45  & 0.5   \\ \hline
ground truth rank                & 3    & 2     & 1     \\ \hline
Shapley score            & 0.13 & 0.59  & 0.28  \\ \hline
Shapley rank                        & 3    & 1     & 2     \\ \hline
Ranks are aligned                & T & F & F \\ \hline
\end{tabular}

Percent of Ranks Aligned = $\frac{1}{3}= 33.33\%$

